# ğŸš—ğŸš—ğŸš— Projet RLCode2-2 : Impact du ParamÃ¨tre Alpha dans Q-Learning ğŸš—ğŸš—ğŸš—

--------------------
# ğŸ® PremiÃ¨re exÃ©cution avec rendu visuel ğŸ®
--------------------

```bash
git clone https://github.com/hrhouma/RLCode2-2.git
cd RLCode2-2
python3 -m venv mountain_car
mountain_car\Scripts\activate  # Sur Windows
source mountain_car/bin/activate  # Sur macOS/Linux
pip install -r requirements.txt
python main1-pygame-alphas.py  # PremiÃ¨re exÃ©cution avec rendu visuel
deactivate
```

--------------------
# ğŸ“Š DeuxiÃ¨me exÃ©cution sans rendu visuel ğŸ“Š
--------------------

```bash
git clone https://github.com/hrhouma/RLCode2-2.git
cd RLCode2-2
python3 -m venv mountain_car
mountain_car\Scripts\activate  # Sur Windows
source mountain_car/bin/activate  # Sur macOS/Linux
pip install -r requirements.txt
python main2-matlplotlib-alphas.py  # DeuxiÃ¨me exÃ©cution sans rendu visuel
deactivate
```

--------------------
# ğŸ” DiffÃ©rences clÃ©s entre les deux fichiers **`main1-pygame-alphas.py`** et **`main2-matplotlib-alphas.py`** :
--------------------

### 1. **DiffÃ©rence principale : Mode d'affichage pendant la simulation**

#### a) **`main1-pygame-alphas.py`** :
- **ğŸ¬ Affichage en temps rÃ©el** : Ce script **affiche la simulation en temps rÃ©el** avec un rendu graphique de la voiture dans l'environnement **MountainCar-v0** pendant que l'agent prend des dÃ©cisions. Cela permet de voir la voiture se dÃ©placer dans l'environnement pendant les essais.
- **ğŸ“º FenÃªtre `gym` rendue** : Utilisation du mode de rendu de **gym** pour visualiser le dÃ©placement de la voiture Ã  chaque Ã©tape dans une fenÃªtre sÃ©parÃ©e.
- **ğŸ¤– Simulation interactive** : Chaque essai se dÃ©roule dans un environnement visuel oÃ¹ vous pouvez observer le comportement de l'agent pendant la simulation. Ensuite, Ã  la fin de chaque essai, les rÃ©sultats sont collectÃ©s pour produire des graphiques rÃ©capitulatifs.

#### b) **`main2-matplotlib-alphas.py`** :
- **â© Simulation sans rendu visuel en temps rÃ©el** : Dans ce script, les simulations sont effectuÃ©es **sans affichage visuel** pendant les essais. L'agent prend des dÃ©cisions et l'environnement est mis Ã  jour, mais il n'y a pas de rendu graphique montrant la voiture en mouvement.
- **ğŸ“Š Affichage Ã  la fin** : Les rÃ©sultats sont uniquement affichÃ©s aprÃ¨s la fin de tous les essais sous forme de graphiques. Vous ne voyez pas le comportement de la voiture pendant les simulations.

### 2. **Utilisation de `pygame` dans `main1`**

- **`main1-pygame-alphas.py`** utilise **`pygame`** et **`gym`** pour afficher la simulation en temps rÃ©el. Cela permet de voir l'agent agir directement dans l'environnement, ce qui est utile pour des dÃ©monstrations interactives oÃ¹ vous souhaitez observer le comportement de l'agent pendant l'apprentissage.
  
- En revanche, **`main2-matplotlib-alphas.py`** n'utilise pas **`pygame`** ou le rendu en temps rÃ©el de **gym**. L'accent est mis uniquement sur la collecte des rÃ©sultats des essais et l'affichage des graphiques une fois que toutes les simulations sont terminÃ©es.

### 3. **ğŸ“ˆ Affichage graphique des rÃ©sultats**

- **Dans les deux scripts** : Ã€ la fin de la simulation (aprÃ¨s les essais), les deux scripts gÃ©nÃ¨rent des graphiques avec **matplotlib** pour rÃ©sumer les rÃ©sultats des simulations. Chaque script crÃ©e des graphiques montrant :
  - âœ… Le succÃ¨s ou l'Ã©chec de chaque essai en fonction de la valeur d'**alpha**.
  - â±ï¸ Le nombre de pas de temps nÃ©cessaires pour chaque essai.
  
- **DiffÃ©rence d'implÃ©mentation** :
  - **Dans `main1-pygame-alphas.py`**, les graphiques sont gÃ©nÃ©rÃ©s aprÃ¨s avoir observÃ© la simulation en temps rÃ©el.
  - **Dans `main2-matplotlib-alphas.py`**, les graphiques sont gÃ©nÃ©rÃ©s immÃ©diatement aprÃ¨s l'exÃ©cution des essais, sans observer le processus en direct.

### 4. **ğŸ¯ ScÃ©narios d'utilisation**

#### a) **`main1-pygame-alphas.py`** :
- **IdÃ©al pour les dÃ©monstrations interactives** oÃ¹ vous souhaitez observer le comportement de l'agent en temps rÃ©el pendant l'apprentissage. Vous pouvez voir comment l'agent explore l'environnement et s'amÃ©liore au fil du temps.

#### b) **`main2-matplotlib-alphas.py`** :
- **Meilleur pour des simulations rapides** oÃ¹ l'affichage en temps rÃ©el n'est pas nÃ©cessaire. Cela permet d'exÃ©cuter les simulations plus rapidement sans ouvrir des fenÃªtres graphiques. Il est utile lorsque vous souhaitez simplement analyser les rÃ©sultats aprÃ¨s l'exÃ©cution des essais.

### 5. **ğŸ“ Conclusion pour les lecteurs** :
- **Si vous voulez observer l'apprentissage de l'agent en temps rÃ©el**, utilisez **`main1-pygame-alphas.py`**. Cela est particuliÃ¨rement utile pour montrer visuellement comment diffÃ©rentes valeurs d'**alpha** affectent le comportement de l'agent dans un environnement interactif.
  
- **Si vous prÃ©fÃ©rez effectuer des simulations rapidement sans affichage en temps rÃ©el**, alors **`main2-matplotlib-alphas.py`** est plus appropriÃ©. Vous aurez directement les rÃ©sultats sous forme de graphiques aprÃ¨s la simulation.

Cela permet aux utilisateurs de choisir le script qui correspond le mieux Ã  leurs besoins, selon qu'ils souhaitent une dÃ©monstration visuelle ou une exÃ©cution rapide sans visualisation.


------------------
# Annexe et InterprÃ©tation
------------------

![Figure_1](https://github.com/user-attachments/assets/5a7fdb88-634c-4a4d-9284-a804a95000d5)


- Le code gÃ©nÃ¨re des visualisations de l'impact de diffÃ©rentes valeurs de **alpha (Î±)** sur la performance d'agents Q-Learning dans l'environnement **MountainCar-v0**. 
- Il compare les rÃ©sultats en termes de succÃ¨s ou d'Ã©checs pour chaque essai, ainsi que le nombre de pas nÃ©cessaires pour chaque essai.

---

### **Structure du Code :**

#### 1. **Boucle d'entraÃ®nement et Ã©valuation :**
   - Le code entraÃ®ne des agents avec trois valeurs de **alpha** : **0.1, 0.5, 0.9**.
   - Pour chaque valeur de **alpha**, l'agent est entraÃ®nÃ© sur **2000 Ã©pisodes** puis Ã©valuÃ© sur **10 essais**.
   - **RÃ©sultats des essais** :
     - Le **taux de succÃ¨s** : Le nombre d'essais oÃ¹ l'agent a atteint son objectif (rÃ©ussir Ã  gravir la colline en moins de 200 pas).
     - Le **nombre de pas** nÃ©cessaires pour atteindre l'objectif ou Ã©chouer.
  
#### 2. **Visualisation des rÃ©sultats :**
   - **Graphiques des succÃ¨s/Ã©checs** : Pour chaque valeur de **alpha**, un graphique indique si l'agent a rÃ©ussi (1) ou Ã©chouÃ© (0) Ã  chaque essai.
   - **Graphiques des pas de temps** : Un autre graphique montre le nombre de pas nÃ©cessaires pour chaque essai. Ces donnÃ©es sont reprÃ©sentÃ©es sous forme de courbes rouges.

---

### **InterprÃ©tation des graphiques :**

#### **Alpha (Î±) = 0.1 :**
   - **Graphique des succÃ¨s (en bleu)** :
     - L'agent a un taux de succÃ¨s de **70%** (7 rÃ©ussites sur 10).
     - Cependant, il Ã©choue Ã  certains essais (entre les essais 4 et 6), ce qui suggÃ¨re que la faible valeur d'**alpha** entraÃ®ne un apprentissage plus lent.
   - **Graphique des pas de temps (en rouge)** :
     - Le nombre de pas varie beaucoup, ce qui indique une certaine instabilitÃ© dans la performance. Par exemple, Ã  l'essai 4, l'agent a besoin de plus de 220 pas, alors qu'Ã  l'essai 10, il rÃ©ussit avec beaucoup moins de pas (~140).

#### **Alpha (Î±) = 0.5 :**
   - **Graphique des succÃ¨s (en bleu)** :
     - Ici, l'agent a un taux de succÃ¨s de **100%**. Il rÃ©ussit tous ses essais, montrant que **Î± = 0.5** est une valeur qui permet un bon compromis entre exploration et exploitation.
   - **Graphique des pas de temps (en rouge)** :
     - Le nombre de pas est plus stable et tend vers une **moyenne de 150 pas** pour chaque essai, avec moins de fluctuations. Cela suggÃ¨re que l'agent a appris une politique efficace avec cette valeur d'alpha.

#### **Alpha (Î±) = 0.9 :**
   - **Graphique des succÃ¨s (en bleu)** :
     - Comme pour **Î± = 0.5**, l'agent a un taux de succÃ¨s de **100%**. Cependant, la performance ne semble pas aussi stable que pour **Î± = 0.5**.
   - **Graphique des pas de temps (en rouge)** :
     - Les rÃ©sultats sont moins stables avec **Î± = 0.9**, avec des variations dans le nombre de pas, bien qu'elles soient moins extrÃªmes que pour **Î± = 0.1**. Ã€ l'essai 3, l'agent prend plus de pas que lors des autres essais (~148 contre ~142 dans les autres).

---

### **Analyse des rÃ©sultats :**
- **Alpha faible (0.1)** : L'agent prend plus de temps Ã  apprendre, et son taux de succÃ¨s est infÃ©rieur, bien qu'il parvienne Ã  rÃ©soudre certains essais. La performance est instable.
- **Alpha modÃ©rÃ© (0.5)** : L'agent rÃ©ussit tous les essais avec un nombre de pas relativement stable. Cette valeur d'**alpha** semble bien Ã©quilibrer exploration et exploitation.
- **Alpha Ã©levÃ© (0.9)** : L'agent rÃ©ussit Ã©galement tous les essais, mais avec une performance moins stable, ce qui peut indiquer une exploration excessive.

### **Conclusion pÃ©dagogique :**
- Les graphiques montrent que **Î± = 0.5** semble Ãªtre la valeur optimale dans cet environnement, offrant un bon compromis entre l'apprentissage rapide et une politique stable. Les valeurs d'alpha trop faibles ou trop Ã©levÃ©es entraÃ®nent des performances moins stables.

Ces graphiques vous permettent de **visualiser l'effet des diffÃ©rentes valeurs de taux d'apprentissage** et de comprendre comment elles influencent l'apprentissage d'un agent **Q-Learning** dans un environnement simulÃ©.
